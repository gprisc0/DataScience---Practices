{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190537ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Jan 12 14:24:27 2025\n",
    "\n",
    "@author: Gabriel Prisco\n",
    "Problema: Detectar Fake News \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c964a",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Importar Bibliotecas:"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk as nlp\n",
    "import sklearn as sk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f6941",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Reading datasets"
   },
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv(\"Fake.csv\")\n",
    "true_df = pd.read_csv(\"True.csv\")\n",
    "# Rows and columns of fake news dataset\n",
    "fake_df.shape\n",
    "fake_df.info()\n",
    "fake_df.describe()\n",
    "fake_df.head()\n",
    "fake_df.isnull().sum()\n",
    "# Rows and columns of fake news dataset\n",
    "true_df.shape\n",
    "true_df.info()\n",
    "true_df.describe()\n",
    "true_df.head()\n",
    "true_df.isnull().sum()\n",
    "# Adding 'Fake' column to our datasets then join them together\n",
    "fake_df['Fake']=1\n",
    "true_df['Fake']=0\n",
    "concat_df=pd.concat([fake_df,true_df],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea091d",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Processamento Textual"
   },
   "outputs": [],
   "source": [
    "concat_df['full_text'] = concat_df['title'] + ' ' + concat_df['subject']\n",
    "\n",
    "\n",
    "# Funções para processamento de texto  \n",
    "def process_text(text):  \n",
    "    # Tokenização  \n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    # Remover stopwords e não alfabéticos  \n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]  \n",
    "    # Lematização  \n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]  \n",
    "    return ' '.join(lemmatized)  \n",
    "\n",
    "# Aplicar o processamento no texto completo  \n",
    "concat_df['processed_text'] = concat_df['full_text'].apply(process_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e272b",
   "metadata": {
    "title": "VETORIZAÇÃO"
   },
   "outputs": [],
   "source": [
    "# dividindo target e features\n",
    "y = concat_df['Fake']\n",
    "x = concat_df['processed_text'] \n",
    "#train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) \n",
    "# vetorizando\n",
    "vectorizer = TfidfVectorizer()  \n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)  \n",
    "x_test_vectorized = vectorizer.transform(x_test)  \n",
    "# Inicializar o modelo e treinar  \n",
    "model = MultinomialNB()  \n",
    "model.fit(x_train_vectorized, y_train)  #treinando\n",
    "# Prever no conjunto de teste  \n",
    "y_pred = model.predict(x_test_vectorized)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103db3d",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Avaliar o modelo"
   },
   "outputs": [],
   "source": [
    "print(\"Acurácia:\", round(accuracy_score(y_test, y_pred)*100,2),'%')\n",
    "print(classification_report(y_test, y_pred))  \n",
    "print(sk.metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7645a72",
   "metadata": {
    "lines_to_next_cell": 3,
    "title": "Visualização:"
   },
   "outputs": [],
   "source": [
    "# Calcular a matriz de confusão  \n",
    "cm = sk.metrics.confusion_matrix(y_test, y_pred)  \n",
    "# Criar um heatmap  \n",
    "plt.figure(figsize=(20, 11))  \n",
    "sns.heatmap(cm, annot=True,fmt='d',\n",
    "            xticklabels=['True', 'Fake'], yticklabels=['True', 'Fake'], cmap='viridis',\n",
    "            annot_kws={\"size\": 24})  \n",
    "plt.title('Matriz de Confusão',fontsize=25)  \n",
    "plt.xlabel('Previsões',fontsize=25)  \n",
    "plt.ylabel('Valores Reais',fontsize=25)  \n",
    "plt.tight_layout()\n",
    "plt.savefig('Confusionmatrix_FakeNews')\n",
    "plt.show()     "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
